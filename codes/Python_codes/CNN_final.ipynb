{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fee47122-d2d9-464d-bcac-7be0cc98795c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000 785\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "dataset=pd.read_csv('/home/atharv/ML_Model_FPGA_Eklavya23/train.csv')\n",
    "dataset.head(10)\n",
    "dataset=np.array(dataset)         # array to store image pixels\n",
    "a, b = dataset.shape                 # gets dimensions of array\n",
    "np.random.shuffle(dataset)               # random shuffling of data (images)\n",
    "print(a,b)\n",
    "test_data=dataset[0:2000].T               # Take 2000 examples from training data file and train model using those . \".T\" is matrix transpose\n",
    "test_data_y=test_data[0]                  # test_data_y is the first row of test_data\n",
    "test_data_x=test_data[1:b]                # test_data_x is the rest of the rows of test_data\n",
    "test_data_x=test_data_x/255              # Normalize the data by dividing by 255\n",
    "\n",
    "train_data=dataset[2000:a].T\n",
    "train_data_y=train_data[0]\n",
    "train_data_x=train_data[1:b]\n",
    "train_data_x=train_data_x/255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ea91c05-6023-4739-8394-c16ef990412f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_x[:,b-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "908e9885-4a84-4a1c-b035-1acaef558e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration no. :  0\n",
      "Iteration no. :  10\n",
      "Iteration no. :  20\n",
      "Iteration no. :  30\n",
      "Iteration no. :  40\n",
      "Iteration no. :  50\n",
      "Iteration no. :  60\n",
      "Iteration no. :  70\n",
      "Iteration no. :  80\n",
      "Iteration no. :  90\n",
      "Iteration no. :  100\n",
      "Iteration no. :  110\n",
      "Iteration no. :  120\n",
      "Iteration no. :  130\n",
      "Iteration no. :  140\n",
      "Iteration no. :  150\n",
      "Iteration no. :  160\n",
      "Iteration no. :  170\n",
      "Iteration no. :  180\n",
      "Iteration no. :  190\n",
      "Iteration no. :  200\n",
      "Iteration no. :  210\n",
      "Iteration no. :  220\n",
      "Iteration no. :  230\n",
      "Iteration no. :  240\n",
      "Iteration no. :  250\n",
      "Iteration no. :  260\n",
      "Iteration no. :  270\n",
      "Iteration no. :  280\n",
      "Iteration no. :  290\n",
      "Iteration no. :  300\n",
      "Iteration no. :  310\n",
      "Iteration no. :  320\n",
      "Iteration no. :  330\n",
      "Iteration no. :  340\n",
      "Iteration no. :  350\n",
      "Iteration no. :  360\n",
      "Iteration no. :  370\n",
      "Iteration no. :  380\n",
      "Iteration no. :  390\n",
      "Iteration no. :  400\n",
      "Iteration no. :  410\n",
      "Iteration no. :  420\n",
      "Iteration no. :  430\n",
      "Iteration no. :  440\n",
      "Iteration no. :  450\n",
      "Iteration no. :  460\n",
      "Iteration no. :  470\n",
      "Iteration no. :  480\n",
      "Iteration no. :  490\n"
     ]
    }
   ],
   "source": [
    "def params():\n",
    "    W1=np.random.rand(10,784)-0.5         #Creates an array(10,784) of weights having random values \n",
    "    b1=np.random.rand(10,1)-0.5\n",
    "    W2=np.random.rand(10,10)-0.5         #Creates an array(10,784) of weights having random values \n",
    "    b2=np.random.rand(10,1)-0.5\n",
    "    return W1,W2,b1,b2\n",
    "\n",
    "def ReLU(Z):                               #defining ReLU function for \n",
    "    return np.maximum(0,Z) #FIX\n",
    "\n",
    "def softmax(Z):                            #softmax converts output into a probability distribution (squish output between 0 & 1)   \n",
    "    exp=np.exp(Z)\n",
    "    return exp/np.sum(exp, axis=0, keepdims=True) #FIX(axis and \n",
    "\n",
    "def for_prop(W1,W2,b1,b2,X):\n",
    "    Z1=W1.dot(X)+b1\n",
    "    # print(W1)\n",
    "    # print(W2)\n",
    "    A1=ReLU(Z1)\n",
    "    Z2=W2.dot(A1)+b2\n",
    "    A2=softmax(Z2)\n",
    "    f=open('Weightsfile.txt','w')\n",
    "    f.write(str(W1))\n",
    "    f.write(str(W2))\n",
    "    f.close()\n",
    "    return Z1,Z2,A1,A2\n",
    "# print(W1)\n",
    "\n",
    "def one_hot(Y,num_classes):                        #FIX(it will take 2 params) one_hot converts output into a vector of probabilities\n",
    "    one_hot_Y=np.zeros((num_classes,Y.size))       #FIX(matrix of size num_classes x Y.size of zeros)\n",
    "    one_hot_Y[Y,np.arange(Y.shape[0])]=1 #FIX\n",
    "    return one_hot_Y\n",
    "\n",
    "def der_ReLU(Z):\n",
    "    return Z>0\n",
    "    \n",
    "def back_prop(Z1,Z2,A1,A2,W1,W2,X,Y):      #checks for how much loss occurs and changes params to minimizes it\n",
    "    one_hot_Y=one_hot(Y,A2.shape[0])       #FIX(one_hot matrix of y rows and A2.shape columns\n",
    "    dZ2=A2-one_hot_Y\n",
    "    a = X.shape[1] #FIX\n",
    "    dW2=1/a * dZ2.dot(A1.T)\n",
    "    db2=1/a * np.sum(dZ2)\n",
    "    dZ1=W2.T.dot(dZ2) * der_ReLU(Z1)\n",
    "    dW1=1/a * dZ1.dot(X.T)\n",
    "    db1=1/a * np.sum(dZ1)\n",
    "    return dW1,dW2,db1,db2\n",
    "\n",
    "def update_params(W1,W2,b1,b2,dW1,dW2,db1,db2,rate):      #rate at which learning occurs (checks error & improves)\n",
    "    W1=W1-rate*dW1\n",
    "    b1=b1-rate*db1\n",
    "    W2=W2-rate*dW2\n",
    "    b2=b2-rate*db2\n",
    "    return W1,W2,b1,b2\n",
    "\n",
    "def predict(A2):\n",
    "    return np.argmax(A2,axis=0)      #FIX(axis needed to be defined 0 , default is none) argmax gives max value of function\n",
    "\n",
    "def accuracy(prediction,Y):\n",
    "    return np.mean(prediction==Y)      #FIX(direct use of mean function rather than taking mean by sum & then divide by size)\n",
    "                                       #mean gives average of all values\n",
    "\n",
    "def gradient(X,Y,rate,iterations):\n",
    "    W1,W2,b1,b2 = params()\n",
    "    for i in range(iterations):\n",
    "        Z1,Z2,A1,A2=for_prop(W1,W2,b1,b2,X)\n",
    "        dW1,dW2,db1,db2=back_prop(Z1,Z2,A1,A2,W1,W2,X,Y)\n",
    "        W1,W2,b1,b2=update_params(W1,W2,b1,b2,dW1,dW2,db1,db2,rate)\n",
    "        if i%10==0:\n",
    "            print(\"Iteration no. : \",i)\n",
    "            prediction=predict(A2)\n",
    "            # print(accuracy(prediction,Y)) #FIX\n",
    "    return W1,W2,b1,b2\n",
    "    \n",
    "W1,W2,b1,b2=gradient(train_data_x,train_data_y,0.10,500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de825b2c-f94b-4a07-b2c3-db51ddd6b074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
